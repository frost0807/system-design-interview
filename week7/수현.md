
# 13장 검색어 자동완성 시스템 설계
구글, 아마존 웹 사이트의 경우 입력 중인 글자에 맞게 검색어가 자동으로 완성되어 표시된다 이런 긴으은 보통 "검색어 자동완성"이라고 부른다.

## 1단계 문제 이해 및 설게 범위 확정
요구사항
* 빠른 응답 속도 : 사용자가 검색어를 입력함에 따라 자동완성 검색어도 충분히 빠르게 표시되어야 한다. 페이스북의 경우 응답속도를 100 밀리초 이내라고 규정한다. 그렇지 않으면 시스템 이용이 불편해진다.
* 연관성 : 검색어는 사용자가 입력한 단어와 연관성이 있어야 한다.
* 정렬 : 시스템의 계산 결과는 인기도 등의 순위 모델에 의해 정렬되어야 한다.
* 규모 확장성 : 시스템은 많은 트래픽을 감당할 수 있도록 확장 가능해야 한다.
* 고가용성 : 시스템의 일부에 장애가 발생하거나 느려지거나, 예상치 못한 네트워크 문제가 발생해도 시스템은 계속 사용 가능해야 한다.

### 개략적 규모 추정
* DAU는 천만 명으로 가정
* 평균적으로 한 사용자는 매일 10건의 검색을 수행한다고 가정
* 질의 할 때 평균적으로 20바이트의 데이터를 입력한다고 가정
* 검색창에 글자를 입력할 때마다 클라이언트는 백엔드 서버로 요청을 보낸다. 따라서 평균적으로 1회 검색당 20건의 요청이 백엔드로 전달된다.
* 대략 초당 24,000건의 질의 발생 = (10,000,000 x 10 질의 / 일 x 20자 / 24시간 / 3600초)
* 질의 가운데 20% 정도는 신규 검색어라고 가정.
 
## 2단계 개략적 설계안 제시 및 동의 구하기
개략적으로 보면 시스템은 크게 데이터 수집 서비스와 질의 서비스로 나눌 수 있다.

데이터 수집 서비스란 사용자가 입력한 질의를 실시간으로 수집하는 시스템이다.  질의한 단어와 횟수를 빈도 테이블에 저장
질의 서비스 : 주어진 질의에 다섯 개의 인기 검색어를 정렬해 놓는 서비스

## 3단계 상세 설계
트라이 자료구조
관계형 데이터베이스를 사용해서 가장 인기 있던 다섯 개 질의문을 골라내는 방법은 효율적이지 않다. 
트라이는 문자열을 찾기 위해 사용되는 자료구조 중 하나이다.
루트 노드는 빈문자열을 가지고 그 밑에 자식 노드에는 포함되는 단어의 첫글자, 그 다음에는 26가지 단어가 올 수 있다. 이런 식으로 단어가 형성되기까지의 가정을 트리 형태로 만들어놓은 자료구조이다.

 

# 14장 유튜브 설계
유튜브의 기능은 단순 비디오 시청 말고도 많은 기능이 있으므로, 면접관과 조율해 범위를 좁혀보도록 한다.

## 1단계 문제 이해 및 설게 범위 확장
개략적 규모 추정
* 일간 능동 사용자(DAU: Daily Active User)수는 5백만(5million)
* 한 사용자는 하루에 평균 5개의 비디오를 시청
* 10%의 사용자가 하루에 1비디오 업로드
* 비디오 평균 크기는 300MB
* 비디오 저장을 위해 매일 새로 요구되는 저장 용량 = 5백만 x 10% x 300MB = 150TB
* CDN비용
- 클라우드 CDN을 통해 비 디오를 서비스할 경우 CDN에서 나가는 데이터의 양에 따라 과금한다.
- 아마존의 클라우드프론트(CloudFront)를 CDN 솔루션으로 사용할 경우, 100% 트래픽이 미국에서 발생한다고 가정하면 1GB당 $0.02의 요금 이 발생한다. 문제를 단순화하기 위해 비디오 스트리밍 비용 만 따지도록 하겠다.
- 따라서 매일 발생하는 요금은 5백만 x 5비디오 x 0.3GB x $0.02 = $150,000 이다.

## 2단계 개략적 설계안 제시 및 동의 구하기
규모 확장이 쉬운 BLOB저장소나 CDN을 만드는 것은 지극히 복잡할 뿐 아니라 많은 비용이 드는 일이다.
넷플릭스나 페이스북 같은 큰 회사도 모든 것을 스스로 구축하지는 않는다. 넷플릭스는 아마존의 클라우드 서비스를 사용하고, 페이스북은 아카마이(Akamai)의 CDN을 이용한다.

### 비디오 업로드 절차
![image](https://github.com/user-attachments/assets/5e415292-69b0-424d-9e5c-b9286bacca66)

### 비디오 스트리밍 절차 
![image](https://github.com/user-attachments/assets/801acf5e-e492-4973-9c00-0569287a3831)
비디오 스트리밍은 CDN을 통해서 이루어진다. 각 단말기마다 가장 가까운 CDN 에서 스트링밍이 이뤄지니 전송지연에대한 문제는 크게 일어나지 않는다.

## 3단계 상세 설계
개략적 설계안에서는 전체 시스템을 비디오 업로 드를 담당하는 부분과 비디오 스트리밍을 담당하는 부분으로 나눠 살펴봤다.
* 최적화 방안
* 오류 처리 메커니즘

## 4단계 마무리
* API 계층의 규모 확장성 확보 방안: API 서버는 무상태 서버이므로 수평적 규모 확장이 가능하다는 사실을 언급
* 데이터베이스 계층의 규모 확장성 확보 방안: 다중화, 샤딩
* 라이브 스트리밍(live streaming)
  우리가 한 건 라이브 스트리밍 시스템은 아니다. 차이점을 꼽자면
- 라이브 스트리밍의 경우에는 웅답지연이 좀 더 낮아야 한다. 따라서 스 트리밍 프로토콜 선정에 유의해야 한다.
- 라이브 스트리밍의 경우 병렬화 필요성은 떨어질 텐데, 작은 단위의 데이 터를 실시간으로 빨리 처 리해야 하기 때문이다.
- 라이브 스트리밍의 경우 오류 처리 방법을 달리해야 한다. 너무 많은 시 간이 걸리는방안은사용하기 어렵다.
* 비디오 삭제(takedown) 정책

# 15장 구글 드라이브 설계
구글 드라이브, 드롭박스, 마이크로소프트, 원드라이브, 애플 아이클라우드 등의 클라우드 저장소 서비스는 최근 높은 인기를 누리게 된 대표적 클라우드 서비스이다.
그 가운데 구글 드라이브 서비스 위주로 설계 해보도록 한다.

구글 드라이브는 파일 저장 및 동기화 서비스로, 문서 사진 비디오 기타 파일을 클라우드에 보관할 수 있도록 한다. 
이 파일은 컴퓨터, 스마트폰, 태블릿 등 어떤 단말에서도 이용 가능해야한다. 또 보관된 파일은 친구,가족, 동료에게 손 쉽게 공유할 수 있어야한다.

## 1단계 문제 이해 및 설게 범위 확정
설계 조건
> 중요하게 지원해야 할 기능:  파일 업로드 다운로드, 파일 동기화, 알림
> 모바일 앱, 웹 둘다 지원
> 파일 암호화 해야 함
> 파일크기 10GB 제한
> 사용자는 일간 능동 사용자 기준으로 천만명

비-기능적 요구 사항
* 안정성 : 데이터 손실 X
* 빠른 동기화 속도: 파일 동기화에 시간이 너무 많이 걸리면 사용자 떠나감
* 네트워크 대역폭 : 불필요하게 너무 많은 네트워크 대역폭을 잡아먹으면 안됨
* 규모 확장성 : 아주 많은 양의 트래픽도 처리가능해야 힘
* 높은 가용성 : 일부 서버 장애가 발생해도 시스템 계속 사용 가능

## 2단계 개략적 설계안 제시 및 동의 구하기

### API
이 시스템에서는 파일 업로드 API, 다운로드 API, 파일 갱신 히스토리 제공 API가 필요하다.\

#### 파일 업로드 API
이 시스템은 두가지 종류의 업로드를 지원한다. 
* 단순 업로드: 파일 크기가 작을 때 
* 이어 올리기: 파일 사이즈가 크고 네트워크 문제로 업로드가 중단될 가능성 높다고 생각되면 사용한다.
  
이어올리기의 경우 다음 세 단계의 절차로 이루어짐
1. 이어올리기 URL을 받기 위한 최초 요청 전송
2. 데이터를 업로드하고 업로드 상태 모니터링
3. 업로드에 장애가 발생하면 장애 발생시점부터 업로드를 재시작

#### 파일 다운로드 API 
```
https://api.example.com/files/download
```
인자는 다운로드할 파일 경로인 path를 받는다.

#### 파일 갱신 히스토리 API
```
https://api.example.com/files/list revisions
```
인자는 갱신 히스토리를 가져올 파일의 경로인 path와 히스토리 길이의 최대치인 limit를 받아온다.

지금까지 나열한 모든 API는 사용자 인증을 필요로 하고 HTTPS 프로토콜을 사용해야한다.

### 한 대 서버의 제약 극복
![image](https://github.com/user-attachments/assets/4d8238ad-a946-4941-8e5d-21cd0fb10fbc)
업로드할 파일이 많아지다보면 예시의 그림처럼 파일 시스템이 가득차게 된다  
이럴 때 가장 먼저 떠오르는 해결책은 데이터를 샤딩하여 여러 서버에 나눠서 저장하는 것이다.

![image](https://github.com/user-attachments/assets/d48b4b88-ad44-43e0-9324-490de170796d)

드라이브 같은 저장소는 아마존 s3를 주로 사용한다. 아마존 s3은 업계 최고 수준의 규모 확장성, 가용성, 보안, 성능을
재공하는 객체 저장소 서비스이다.
S3은 다중화를 지원해 같은 지역 안에서 다중화 할 수도 있고 여러 지역에 걸쳐서 다중화 할 수 도 있다.
AWS 지역은 아마존 AWS가 데이터 센터를 운영하는 지리적 영역이다. 데이터를 다중화 할떄는 같은 지역안에서만 할 수도 있고 
여러지역에 걸쳐 데이터 손실을 막고 가용성을 최대한 보장 하는 방식으로 진행 할 수 있다. 
![image](https://github.com/user-attachments/assets/891a62ab-f2d3-4636-b035-366e820cfacf)

이런 방식으로 데이터를 S3에 넣으면 데이터 손실에 대한 걱정을 덜 수 있지만 더 개선해야하는 부분이 있다.
* 로드밸런서: 네트워크 트래픽 분산을 위해 사용, 특정 웹 서버에 장애 발생 시 자동으로 해당 서버 우회
* 웹 서버: 로드 밸런서 사용 시 서버 수 확장 가능
* 메타데이터 데이터베이스: 데이터베이스를 파일 저장 서버에서 분리하여 SPOF를 회피, 다중화 및 샤딩 정책을 적용
* 파일 저장: S3를 파일 저장소로 사용, 가용성과 데이터 무손실을 보장하기 위해 두 개 이상의 지역에 데이터 다중화

![image](https://github.com/user-attachments/assets/202cb242-1f6e-43aa-b34d-453b2b754904)

### 동기화 충돌
구글 드라이브같은 대형 저장소 시스템의 경우는 때때로 동기화 충돌이 발생한다. 이런 경우는 사용자가 동시에 같은 파일이나
폴더를 올리려고 할 떄 발생한다. 이럴 때는 오류가 발생한 시점에 이 시스템에는 같은 파일의 두가지 버전이 존재하게 되는데
이 상태에서 두 개의 파일을 합칠지 하나로 대체할지 결정해야 한다.
![image](https://github.com/user-attachments/assets/5003368c-0d7d-47fd-90de-728c456d57e7)

### 개략적 설계안
![image](https://github.com/user-attachments/assets/ea16f7c9-d704-401b-8f78-89d57b6a6d2f)

* 블록 저장소 서버
  * 파일 블록을 클라우드 저장소에 업로드하는 서버
  * 파일을 여러 개의 블록으로 나눠 저장하며, 각 블록에는 고유한 해시값 할당
  * 해시값을 메타데이터 데이터베이스에 저장

* 클라우드 저장소
  * 파일은 블록 단위로 나눠져 클라우드 저장소에 보관

* 아카이빙 저장소
  * 오랫동안 사용되지 않은 비활성 데이터를 저장하기 위한 시스템

* 로드밸런서
  * 요청을 모든 API 서버에 고르게 분산

* API 서버
  * 파일 업로드 외 거의 모든 것 담당
  * 사용자 인증, 사용자 프로파일 관리, 파일 메타데이터 갱신 등

* 메타데이터 데이터베이스
  * 사용자, 파일, 블록, 버전 등의 메타데이터 정보를 관리
  * 실제 파일은 클라우드에 저장하고 메타데이터만 저장

* 메타데이터 캐시
  * 자주 쓰이는 메타데이터는 캐시한다.

* 알림 서비스
  * 특정 이벤트가 발생했음을 클라이언트에게 알리는데 쓰이는 발생/구독 프로토콜 기반 시스템

* 오프라인 사용자 백업 큐
  * 클라이언트가 접속 중이 아니라서 파일의 최신 상태를 확인할 수 없을 때는 해당 정보를 이 큐에 두어 나중에 클라이언트가 접속했을 때 동기화될 수 있도록 한다

## 3단계 상세 설계

### 블록 저장소 서버
정기적으로 갱신해야하는 큰 파일들을 업데이트 할 떄마다 전체 파일을 서버로 보내게 되면 네트워크 대역폭을 너무 잡아먹게 된다. 이를 최적화하는 방법은 두가지다.
* 델타 동기화 : 파일이 숮어되면 전체 파일 대신 수정이 일어난 블록만 동기화 하는것
* 압축 : 블록 단위로 압축해 데이터 크기를 줄인다. 이 때 압축 알고리즘은 파일 유형에 따라 정한다.

블록 저장소 서비스는 클라이언트가 보낸 파일들을 블록단위로 나누고 각 블록에 맞게 압축 알고리즘을 적용해야하며, 암호화까지 해야하기 때문에 많은 업무를 담당하고 있다.
아울러, 전체 파일을 저장소 시스템에 보내는 대신 수정된 블록까지 보내는 기능도 담당하고 있다. 

이러한 과정을 거쳐서 클라우드 저장소에 파일을 전송한다.
![image](https://github.com/user-attachments/assets/0b80bdbf-33a4-47d7-8479-11eeb8a85d7f)
주어진 파일을 작게 블록으로 분할> 각 블록 압축> 클라우드 저장소 보내기 전 암호화> 클라우드 저장소 보내기

델타 동기화 전략의 동작 과정
![image](https://github.com/user-attachments/assets/d518624a-725c-4503-a706-32c16f81aec6)
갱신된 부분만 동기화 하기 때문에 이 두 블록만 클라우드 저장소에 업로드한다.

### 높은 일관성 요구사항
같은 파일이 단말이나 사용자에 따라 다르게 보이는 것은 허용할 수 없기에 강한 일관성 모델을 기본으로 지원해야한다.
메타데이터 캐시나 데이터 베이스 계층에도 동일하다.
메모리 캐시는 보통 결과적 일관성 모델을 지원한다. 
* 캐시에 보관된 사본과 데이터베이스 원본 일치해야한다.
* 데이터베이스에 보관된 원본을 변경하면 캐시에 있는 사본이 무효화되어야한다.

관계형 데이터 베이스는 ACID를 보장하기 때문에 강한 일관성을 보장하기 쉽다. 하지만 NOSQL 데이터베이스는 지원하지 않기 때문에 동기화 로직을 짜줘야한다.

### 메타데이터 데이터베이스
![image](https://github.com/user-attachments/assets/9e0cf776-2084-40ee-8d6a-4caee424d9e0)

* 테이블 정보
  * user
    * 이름, 이메일, 프로파일 사진 등 사용자에 관계된 기본 정보
  * device
    * 단말 정보
    * push_id는 모바일 푸시 알림을 보내고 받기 위한 것
  * namespace
    * 사용자의 루트 디렉터리 정보
  * file
    * 파일의 최신 정보
  * file_version
    * 파일의 갱신 이력이 보관되는 테이블
    * 이 테이블에 저장되는 레코드는 전부 읽기 전용
  * block
    * 파일 블록에 대한 정보 보관
    * 특정 버전의 파일은 파일 블록을 올바른 순서로 조합 시 복원 가능

### 업로드 절차
메타데이터 추가 요청과 클라우드 저장소 업로드 요청이 병렬적으로 전송
![image](https://github.com/user-attachments/assets/1682d37d-873f-4e07-aa49-2393982c1779)

### 다운로드 절차
파일 다운로드는 파일이 새로 추가되거나 편집되면 자동으로 시작된다. 
* 클라이언트 A가 접속중이고 다른 클라이언트가 파일 변경 시 알림 서비스가 클라이언트 A 에게 변경이 발생했으니 새 버전으로 끌어와야한다고 알린다.
* 클라이언트 A가 네트워크에 연결되어있지 않을 경우 데이터는 캐시에 보관된다. 해당 클라이언트가 접속중으로 바뀌면 그때 해당 클라이언트는 새 버전을 가져온다.
  
![image](https://github.com/user-attachments/assets/caaf6180-7330-4a17-a50f-8a6020251826)
1. 알림 서비스가 클라이언트2에게 누군가 파일 변경했음을 알림
2. 알림을 확인한 클라이언트2는 새로운 메타데이터를 요청하여 받아옴
3. 클라이언트2는 메타데이터를 받자마자 블록 다운로드 요청 전송
4. 블록 저장소 서버는 요청한 블록을 클라우드 저장소에서 가져와서 반환
5. 클라이언트2는 전송된 블록을 사용하여 파일 재구성

### 알림 서비스
단순하게 보자면 알림서비스는 이벤트 데이터를 클라이언트들로 보내는 서비스이다.
* 롱 폴링: 드롭박스가 채택힌 방식
* 웹 소켓: 클라이언트와 서버 사이에 지속적인 통신 채널을 제공한다. 따라서 양방향 통신 가능

* 롤 폴링이 좋은 이유
  * 채팅 서비스와 달리 본 시스템의 경우는 알림서비스와 양방향 통신이 필요하지 않다. 서버는 파일이 변경된 사실을 클랄이언트에게 알려줘야하지만, 반대 방향의 통신은 필요X
  * 롱 폴링을 쓰게 되면 각 클라이언트는 알림 서버와 롱 폴링 용 연결을 유지하다가 파일 변경 감지 시 연결을 끊음 → 클라이언트는 메타데이터 서버와 연결해 파일의 최신 내역 다운로드 다운로드 끝 or 타임아웃 시 즉시 새 요청을 보내어 롱 폴링 연결 복원 및 유지

### 저장소 공간 절약
파일 갱신 이력을 보존하고 안정성을 보장하기 위해서는 파일의 여러 버전을 여러 데이터센터에 저장해야 한다. 하지만 이렇게 하면 저장용량이 빠르게 소진될 수 있기 때문에
아래와 같은 방법을 사용한다.
* 중복제거 : 해시값을 비교해 중복된 파일 블록을 계정 차원에서 제거
* 지능적 백업 전략 :한도 설정, 중요한 버전만 보관
* 자주 쓰이지 않는 데이터는 아카이빙 저장소로 옮긴다.

### 장애 처리
* 로드밸런서 장애 : 로드 밸런서끼리 보통 박동(heartbeat) 신호를 보내서 상태를 모니터링 함
* 블록 저장소 서버 장애: 클라우드 저장소 장애
* API 서버 장애 : API서버들은 무상태 서버. 로드밸런서는 API서버에 장애가 발생하면 트래픽을 해당 서버로 보내지 않음으로써 장애 서버를 격리할 것
* 메타데이터 캐시 장애 
* 메타데이터 데이터베이스 장애
* 알림 서비스 장애 : 한 대 서버에 장애가 발생하면 수만 명 이상의 사용자가 롱 폴링 연결을 다시 만들어야 함 - 복구 시 시간 오래 걸림
* 오프라인 사용자 백업 큐 장애 : 큐에 장애가 발생하면 구독 중인 클라이언트 들은 백업 큐로 구독 관계를 재설정해야함.

## 4단계 마무리
블록 저장소 서버를 거치지 않고 파일을 클라우드 저장소에 직접 업로드 한다면?
 * 장점 : 업로드시간이 빨라진다
 * 단점
   * 분할 압축 암호화 로직을 클라이언트에 두어야하기 때문에 플랫폼별로 따로 구현해야 함
   * 클라이언트가 해킹 당한 가능성이 있으므로 암호화 로직을 클라이언트 안에 두는 것은 적절치 않다.
